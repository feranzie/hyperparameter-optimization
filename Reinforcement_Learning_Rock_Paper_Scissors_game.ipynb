{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feranzie/hyperparameter-optimization/blob/main/Reinforcement_Learning_Rock_Paper_Scissors_game.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This module generate one rock-paper-sessiors move based on the Mode selected.\n",
        "import random\n",
        "\n",
        "def lfsr2(seed, taps, nbits):\n",
        "    sr = seed\n",
        "    while 1:\n",
        "        xor = 1\n",
        "        for t in taps:\n",
        "            if (sr & (1<<(t-1))) != 0:\n",
        "                xor ^= 1\n",
        "        sr = (xor << nbits-1) + (sr >> 1)\n",
        "        yield xor, sr\n",
        "        if sr == seed:\n",
        "            break\n",
        "\n",
        "def genOneMove(self, mode, stage):\t\t\t\t\n",
        "\tif mode == 'PRNG':\n",
        "\t\t# change play strategy for player2 along the way \n",
        "\t\tlowPlay  = {0:0, 1:1, 2:2, 3:0, 4:2} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
        "\t\tmeanPlay = {0:1, 1:2, 2:0, 3:2, 4:1} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
        "\t\thiPlay   = {0:2, 1:0, 2:1, 3:1, 4:0} \t\t\t# key = stage number, value = r(0), p(1), s(2)\n",
        "\t\t# gen a random numbe from guassian & quantize it\n",
        "\t\ta = random.gauss(self.norm_mu, self.norm_sigma) \n",
        "\t\tif a **2 < 1:  \t\t\t\t\t\t\t\t\t# the middle bell is the paper move\n",
        "\t\t\tplay = meanPlay[stage]\n",
        "\t\telif a < -1:\t\t\t\t\t\t\t\t\t# lower than cutoff -1 is the rock move\n",
        "\t\t\tplay = lowPlay[stage]\n",
        "\t\telse:\n",
        "\t\t\tplay = hiPlay[stage] \t\t\t\t\t\t# else higher than +1 is the sessiors move \n",
        "\t\treturn play\n",
        "\t\n",
        "\telif mode == 'SEQ':\t\t\t\t\t# simple repeating pattern as 'random generator'\n",
        "\t\tdict = {'r':0, 'p':1, 's': 2}\n",
        "\t\tseqlist = 'rpprsspsrsrpprspsprspsppsrrspsprrsspsrpsrpsrsps'\t\t\t# the pattern sequence here\n",
        "\t\tself.seqIndex = 0 if self.seqIndex == len(seqlist)-1 else self.seqIndex + 1\n",
        "\t\treturn dict[seqlist[self.seqIndex]]\n",
        "\t\n",
        "\telif mode == 'LFSR':\n",
        "\t\tnbits, tapindex, seed = 12, (12,11,10,4,1), 0b11001001\n",
        "\t\t#nbits, tapindex, seed = 8, (8,6,5,4,1), 0b11001001\n",
        "\t\tlfsrlist = []\n",
        "\t\tfor xor, sr in lfsr2(seed, tapindex, nbits):\n",
        "\t\t    lfsr_gen = int(bin(2**nbits+sr)[3:], base=2)\n",
        "\t\t    lfsrlist.append(lfsr_gen % 3)\n",
        "\t\tself.seqIndex = 0 if self.seqIndex == len(lfsrlist)-1 else self.seqIndex + 1\n",
        "\t\treturn lfsrlist[self.seqIndex]\n",
        "\t\n",
        "\telse:\n",
        "\t\tprint('Error: random mode does not exist!')\n"
      ],
      "metadata": {
        "id": "n3NTqz1uCShE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "\n",
        "# -------------------------- SETTING UP THE ENVIRONMENT --------------------------------------\n",
        "# simple game, therefore we are not using the open gym custom set up\n",
        "#---------------------------------------------------------------------------------------------\n",
        "class RPSenv():\n",
        "\tdef __init__ (self):\n",
        "\t\tself.action_space = [0,1,2]\t\t# integer representation of r/p/s\n",
        "\t\tself.seed = random.seed(4) \t\t# make it deterministic\n",
        "\t\tself.norm_mu = 0\t\t\t\t# center point for guassian distribution\n",
        "\t\tself.norm_sigma = 2.0\t\t\t# sigma for std distribution \n",
        "\t\tself.seqIndex = 0\t\t\t\t# index for pointing to the SEQ sequnce \n",
        "\t\tself.p2Mode = 'PRNG'  \t\t\t# SEQ or PRNG or LFSR\n",
        "\t\tself.p2Count = [0, 0, 0] \t\t# player 2 win tie lost count\n",
        "\t\tself.p1Count = [0, 0, 0]\t\t# player 1 win tie lost count\n",
        "\t\tself.window = 10\t\t\t\t\t# window size for rate trending calc\n",
        "\t\tself.cumWinRate, self.cumTieRate, self.cumLostRate = None, None, None\n",
        "\t\tself.cumWinCount, self.cumTieCount, self.cumLostCount = None, None, None\n",
        "\t\tself.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
        "\t\tself.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = 0, 0, 0\n",
        "\t\tself.winRateBuf, self.tieRateBuf, self.lostRateBuf \\\n",
        "\t\t\t= deque(maxlen=self.window), deque(maxlen=self.window), deque(maxlen=self.window)\n",
        "\t\t# put all the observation state in here; shape in Keras input format\n",
        "\t\tself.state = np.array([[ \\\n",
        "\t\t\tNone, None, None, \\\n",
        "\t\t\tself.winRateTrend, self.tieRateTrend, self.lostRateTrend, \\\n",
        "\t\t\tself.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg \\\n",
        "\t\t\t]])  \n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t# reset all the state\n",
        "\t\tself.cumWinRate, self.cumTieRate, self.cumLostRate = 0, 0, 0\n",
        "\t\tself.cumWinCount, self.cumTieCount, self.cumLostCount = 0, 0, 0\n",
        "\t\tself.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
        "\t\tself.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = 0, 0, 0\n",
        "\t\treturn np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "\tdef step(self, action, moveCount, stage):\t\n",
        "\t\t# value mode is PRNG or SEQ\n",
        "\t\tp2Move = genOneMove(self, self.p2Mode, stage)\t\t\t# play one move from player2\n",
        "\t\tself.p2Count[p2Move] += 1\n",
        "\t\tp1Move = action\n",
        "\t\tself.p1Count[p1Move] += 1\n",
        "\n",
        "\t\t# check who won, set flag and assign reward \n",
        "\t\twin, tie, lost = 0, 0, 0\n",
        "\t\tif p1Move == p2Move:\n",
        "\t\t\tself.cumTieCount, tie   = self.cumTieCount  + 1, 1\n",
        "\t\telif (p1Move - p2Move == 1) or (p1Move - p2Move == -2):\n",
        "\t\t\tself.cumWinCount, win   = self.cumWinCount  + 1, 1\n",
        "\t\telse:\n",
        "\t\t\tself.cumLostCount, lost = self.cumLostCount + 1, 1\n",
        "\n",
        "\t\t# update the running rates \n",
        "\t\tself.cumWinRate = self.cumWinCount / moveCount\n",
        "\t\tself.cumTieRate = self.cumTieCount / moveCount\n",
        "\t\tself.cumLostRate = self.cumLostCount / moveCount\n",
        "\t\t# update moving avg buffer\n",
        "\t\tself.winRateBuf.append(self.cumWinRate) \n",
        "\t\tself.tieRateBuf.append(self.cumTieRate)\n",
        "\t\tself.lostRateBuf.append(self.cumLostRate)\n",
        "\t\t# calculate trend\n",
        "\t\ttmp = [0, 0, 0]\n",
        "\t\tself.winRateTrend, self.tieRateTrend, self.lostRateTrend = 0, 0, 0\n",
        "\t\tif moveCount >= self.window:\n",
        "\t\t\ttmp[0] = sum(self.winRateBuf[i] for i in range(self.window)) / self.window\n",
        "\t\t\ttmp[1] = sum(self.tieRateBuf[i] for i in range(self.window)) / self.window\n",
        "\t\t\ttmp[2] = sum(self.lostRateBuf[i] for i in range(self.window)) / self.window\n",
        "\t\t\t# win rate trend analysis\n",
        "\t\t\tif self.winRateMovingAvg  < tmp[0]: \n",
        "\t\t\t\tself.winRateTrend = 1\t\t# win rate trending up. That's good\n",
        "\t\t\telse: \n",
        "\t\t\t\tself.winRateTrend = 0\t\t# win rate trending down. That's bad\n",
        "\t\t\t# tie rate trend analysis\n",
        "\t\t\tif self.tieRateMovingAvg  < tmp[1]:\n",
        "\t\t\t\tself.tieRateTrend = 1  \t\t# tie rate trending up. That's bad\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.tieRateTrend = 0  \t\t# tie rate trending down.  Neutral\n",
        "\t\t\t# lost rate trend analysis\n",
        "\t\t\tif self.lostRateMovingAvg  < tmp[2]:\n",
        "\t\t\t\tself.lostRateTrend = 1  \t# lst rate trending up.  That's bad\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.lostRateTrend = 0  \t# lost rate trending down. That's good\n",
        "\t\t\tself.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg = tmp[0], tmp[1], tmp[2]\n",
        "\t\t# net reward in this round\n",
        "\t\treward = win \t\t\t\t\t\t\t\t\t\n",
        "\t\t# record the state and reshape it for Keras input format\n",
        "\t\tdim = self.state.shape[1]\n",
        "\t\tself.state = np.array([\\\n",
        "\t\t\twin, tie, lost, \\\n",
        "\t\t\tself.winRateTrend, self.tieRateTrend, self.lostRateTrend, \\\n",
        "\t\t\tself.winRateMovingAvg, self.tieRateMovingAvg, self.lostRateMovingAvg \\\n",
        "\t\t\t]).reshape(1, dim)\n",
        "\t\t# this game is done when it hits this goal\n",
        "\t\tdone = False \n",
        "\t\treturn self.state, reward, done, dim\n",
        "\n",
        "# ------------------------- class for the Double-DQN agent ---------------------------------\n",
        "# facilities utilized here:\n",
        "# 1)  Double DQN networks: one for behavior policy, one for target policy\n",
        "# 2)  Learn from  sample from pool of memories \n",
        "# 3)  Basic TD-Learning stuff:  learning rate,  gamma for discounting future rewards\n",
        "# 4)  Use of epsilon-greedy policy for controlling exploration vs exploitation\n",
        "#-------------------------------------------------------------------------------------------\n",
        "class DDQN:\n",
        "    def __init__(self, env):\n",
        "        self.env     = env\n",
        "        # initialize the memory and auto drop when memory exceeds maxlen\n",
        "        # this controls how far out in history the \"expeience replay\" can select from\n",
        "        self.memory  = deque(maxlen=2000)   \n",
        "        # future reward discount rate of the max Q of next state\n",
        "        self.gamma = 0.9 \t\t\t  \n",
        "        # epsilon denotes the fraction of time dedicated to exploration (as oppse to exploitation)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9910\n",
        "        # model learning rate (use in backprop SGD process)\n",
        "        self.learning_rate = 0.005\t\n",
        "        # transfer learning proportion contrl between the target and action/behavioral NN\n",
        "        self.tau = .125 \t\t\t\n",
        "        # create two models for double-DQN implementation\n",
        "        self.model        = self.create_model()\n",
        "        self.target_model = self.create_model()\n",
        "        # some space to collect TD target for instrumentaion\n",
        "        self.TDtargetdelta, self.TDtarget = [], []\n",
        "        self.Qmax =[]\n",
        "\n",
        "    def create_model(self):\n",
        "        model   = Sequential()\n",
        "        state_shape  = self.env.state.shape[1]\n",
        "        model.add(Dense(24, input_dim=state_shape, activation=\"relu\"))\n",
        "        model.add(Dense(24, activation=\"relu\"))\n",
        "        model.add(Dense(24, activation=\"relu\"))\n",
        "\t\t# let the output be the predicted target value.  NOTE: do not use activation to squash it!\n",
        "        model.add(Dense(len(self.env.action_space)))  \n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
        "        print(model.summary())\n",
        "\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "    \t# this is to take one action\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "        # decide to take a random exploration or make a policy-based action (thru NN prediction)\n",
        "        if np.random.random() < self.epsilon:\n",
        "        \t# return a random move from action space\n",
        "        \treturn random.choice(self.env.action_space)\n",
        "        else:\n",
        "        \t# return a policy move\n",
        "        \tself.Qmax.append(max(self.model.predict(state)[0]))\n",
        "        \treturn np.argmax(self.model.predict(state)[0])\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "\t\t# store up a big pool of memory\n",
        "        self.memory.append([state, action, reward, new_state, done])\n",
        "\n",
        "    def replay(self):  \t\t# DeepMind \"experience replay\" method\n",
        "    \t# the sample size from memory to learn from\n",
        "        batch_size = 32\n",
        "        # do nothing untl the memory is large enough\n",
        "        if len(self.memory) < batch_size: return\n",
        "        # get the samples\n",
        "        samples = random.sample(self.memory, batch_size)\n",
        "        # do the training (learning); this is DeepMind tricks of using \"Double\" model (Mnih 2015)\n",
        "        for sample in samples:\n",
        "            state, action, reward, new_state, done = sample\n",
        "            target = self.target_model.predict(state)\n",
        "            #print('target at state is ', target)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                Q_future = max(self.target_model.predict(new_state)[0]) \n",
        "                TDtarget = reward + Q_future * self.gamma\n",
        "                self.TDtarget.append(TDtarget)\n",
        "                self.TDtargetdelta.append(TDtarget - target[0][action])\n",
        "                target[0][action] = TDtarget\t \t\t\t\n",
        "            # do one pass gradient descend using target as 'label' to train the action model\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        \n",
        "    def target_train(self):\n",
        "    \t# transfer weights  proportionally from the action/behave model to the target model\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "\n",
        "    def save_model(self, fn):\n",
        "        self.model.save(fn)\n",
        "\n",
        "# ------------------------- MAIN BODY ----------------------------------------\n",
        "\n",
        "def main():\n",
        "\tepisodes, trial_len =  150, 300\t\t\t\t\t# lenght of game play\n",
        "\tstage, totalStages = 0, 5\t\t\t\t\t\t# # of stages with change distribution\n",
        "\tsigma_reduce = -0.1\t\t\t\t\t\t\t\t# sigma change amount at each stage\n",
        "\tcumReward, argmax = 0, 0\t\t\t\t\t\t# init for intrumentation\n",
        "\tsteps, rateTrack = [], []\n",
        "\tavgQmaxList, avgQ_futureList,avgQ_targetmaxList, avgTDtargetList = [], [], [], []\n",
        "\tavgCumRewardList = []\n",
        "\tp1Rate, p2Rate = [], []\n",
        "\t# declare the game play environment and AI agent\n",
        "\tenv = RPSenv()\n",
        "\tdqn_agent = DDQN(env=env)\n",
        "\t# ------------------------------------------ start the game -----------------------------------------\n",
        "\tprint('STARTING THE GAME with %s episodes each with %s moves' % (episodes, trial_len), '\\n')\n",
        "\tfor episode in range(episodes):\n",
        "\t\tcur_state = env.reset().reshape(1,env.state.shape[1])   # reset and get initial state in Keras shape\n",
        "\t\tcumReward = 0\n",
        "\t\t# ----------------- Select play strategy (apply to PRNG mode only) ------------------------------\n",
        "\t\t#   this change strategy affects distribution of r-p-s\n",
        "\t\t# -----------------------------------------------------------------------------------------------\n",
        "\t\tif (episode+1) % (episodes // totalStages) == 0: env.norm_sigma += sigma_reduce\t# step repsonse change the gaussian distribution\n",
        "\t\t#   this change strategy affects which move is dominant (control in randmove module)\n",
        "\t\tstage = episode // (episodes // totalStages)\t\t\t\t\t\t\t\t\t# divide total episodes into 3 equal length stages\n",
        "\t\t\n",
        "\t\tfor step in range(trial_len):\n",
        "\t\t\t# AI agent take one action\n",
        "\t\t\taction = dqn_agent.act(cur_state)\n",
        "\t\t\t# play the one move and see how the environment reacts to it\n",
        "\t\t\tnew_state, reward, done, info = env.step(action, step + 1, stage)\n",
        "\t\t\tcumReward += reward\n",
        "\t\t\t# record the play into memory pool\n",
        "\t\t\tdqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "\t\t\t# perform Q-learning from using |\"experience replay\": learn from random samples in memory\n",
        "\t\t\tdqn_agent.replay()\n",
        "            # apply tranfer learning from actions model to the target model.\n",
        "\t\t\tdqn_agent.target_train() \n",
        "\t\t\t# update the current state with environment new state\n",
        "\t\t\tcur_state = new_state\n",
        "\t\t\tif done:  break\n",
        "\t\t#-------------------------------- INSTRUMENTAL AND PLOTTING -------------------------------------------\n",
        "\t\t# the instrumental are performed at the end of each episode\n",
        "\t\t# store epsiode #, winr rate, tie rate, lost rate, etc. etc.\n",
        "\t\t#------------------------------------------------------------------------------------------------------\n",
        "\t\trateTrack.append([episode+1, env.cumWinRate, env.cumTieRate, env.cumLostRate])\n",
        "\t\tif True:\t\t# print ongoing performance\n",
        "\t\t\tprint('EPISODE ', episode + 1), \n",
        "\t\t\tif env.p2Mode == 'PRNG': print('stage:', stage, ' sigma:', env.norm_sigma)\n",
        "\t\t\tprint(' WIN RATE %.2f ' % env.cumWinRate, \\\n",
        "\t\t\t\t'    tie rate %.2f' % env.cumTieRate, \\\n",
        "\t\t\t\t'lose rate %.2f' % env.cumLostRate)\n",
        "\t\t\n",
        "\t\t# print move distribution between the players\n",
        "\t\tif True:\n",
        "\t\t\tp1Rate.append([env.p1Count[0] / trial_len, env.p1Count[1] / trial_len, env.p1Count[2] / trial_len])\n",
        "\t\t\tp2Rate.append([env.p2Count[0] / trial_len, env.p2Count[1] / trial_len, env.p2Count[2] / trial_len])\n",
        "\t\t\tprint (' P1 rock rate: %.2f paper rate: %.2f scissors rate: %.2f' %  (p1Rate[-1][0], p1Rate[-1][1], p1Rate[-1][2]))\n",
        "\t\t\tprint (' P2 rock rate: %.2f paper rate: %.2f scissors rate: %.2f' %  (p2Rate[-1][0], p2Rate[-1][1], p2Rate[-1][2]))\n",
        "\t\t\tenv.p1Count, env.p2Count = [0,0,0], [0,0,0]\n",
        "\t\t\n",
        "\t\t# summarize Qmax from action model and reward \n",
        "\t\tavgQmax = sum(dqn_agent.Qmax) / trial_len  \t# from action model\n",
        "\t\tavgQmaxList.append(avgQmax)\n",
        "\n",
        "\t\tavgCumReward = cumReward / trial_len\n",
        "\t\tavgCumRewardList.append(avgCumReward)\n",
        "\t\tif True:\n",
        "\t\t\tprint(' Avg reward: %.2f Avg Qmax: %.2f' % (avgCumReward, avgQmax))\n",
        "\t\tdqn_agent.Qmax=[] \t\t# reset for next episode\n",
        "\n",
        "\n",
        "\t# ---------------- plot the main plot when all the episodes are done ---------------------------\n",
        "\t#\n",
        "\tif True:\n",
        "\t\tfig = plt.figure(figsize=(12,5))\t\n",
        "\t\tplt.subplots_adjust(wspace = 0.2, hspace = 0.2)\n",
        "\t\t\n",
        "\t\t# plot the average Qmax\n",
        "\t\trpsplot = fig.add_subplot(321)\n",
        "\t\tplt.title('Average Qmax from action model', loc='Left', weight='bold', color='Black', \\\n",
        "\t\t\tfontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot(avgQmaxList, color='blue')\n",
        "\t\t\n",
        "\t\t# plot the TDtarget\n",
        "\t\trpsplot = fig.add_subplot(323)\n",
        "\t\tplt.title('TD target minus Q target from experience replay', loc='Left', weight='bold', \\\n",
        "\t\t\tcolor='Black', fontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot(dqn_agent.TDtarget, color='blue')\n",
        "\t\t\n",
        "\t\t# plot the TDtarget\n",
        "\t\trpsplot = fig.add_subplot(325)\n",
        "\t\tplt.title('TD target from experience replay', loc='Left', weight='bold', color='Black', \\\n",
        "\t\t\tfontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot(dqn_agent.TDtargetdelta, color='blue')\n",
        "\t\t\n",
        "\t\t# plot thte win rate\n",
        "\t\trpsplot = fig.add_subplot(322)\n",
        "\t\tplt.title('Win-Tie-Lost Rate', loc='Left', weight='bold', color='Black', \\\n",
        "\t\t\tfontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot([i[1] for i in rateTrack], color='green')\n",
        "\t\trpsplot.plot([i[2] for i in rateTrack], color='blue')\n",
        "\t\trpsplot.plot([i[3] for i in rateTrack], color='red')\n",
        "\t\t\n",
        "\t\t# plot thte win rate\n",
        "\t\trpsplot = fig.add_subplot(324)\n",
        "\t\tplt.title('Player 2 move percentage', loc='Left', weight='bold', color='Black', \\\n",
        "\t\t\tfontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot([i[0] for i in p2Rate], color='orange')\n",
        "\t\trpsplot.plot([i[1] for i in p2Rate], color='red')\n",
        "\t\trpsplot.plot([i[2] for i in p2Rate], color='green')\n",
        "\t\t\n",
        "\t\t# plot the reward \n",
        "\t\trpsplot = fig.add_subplot(326)\n",
        "\t\tplt.title('Average Reward per Episode', loc='Left', weight='bold', color='Black', \\\n",
        "\t\t\tfontdict = {'fontsize' : 10})\n",
        "\t\trpsplot.plot(avgCumRewardList, color='green')\n",
        "\t\tplt.show(block = False)\n",
        "\t\n",
        "if __name__ == \"__main__\":\n",
        "\tmain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_QFQlYP0_9LT",
        "outputId": "e0a0e0ee-7cf0-474d-a333-1dd58272857f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 24)                240       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 75        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,515\n",
            "Trainable params: 1,515\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 24)                240       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 75        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,515\n",
            "Trainable params: 1,515\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "STARTING THE GAME with 150 episodes each with 300 moves \n",
            "\n",
            "EPISODE  1\n",
            "stage: 0  sigma: 2.0\n",
            " WIN RATE 0.33      tie rate 0.33 lose rate 0.34\n",
            " P1 rock rate: 0.36 paper rate: 0.22 scissors rate: 0.42\n",
            " P2 rock rate: 0.26 paper rate: 0.36 scissors rate: 0.38\n",
            " Avg reward: 0.33 Avg Qmax: 1.33\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c16709360aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-c16709360aba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                         \u001b[0;31m# perform Q-learning from using |\"experience replay\": learn from random samples in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;31m# apply tranfer learning from actions model to the target model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c16709360aba>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTDtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# do one pass gradient descend using target as 'label' to train the action model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 361\u001b[0;31m         grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2022\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m           \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2024\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5237\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5238\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m     \"\"\"\n\u001b[1;32m   3070\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3071\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3072\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3036\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3037\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1074\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[0;32m-> 1076\u001b[0;31m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[1;32m   1077\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[1;32m   1078\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[0;34m(args, names, flat_shapes)\u001b[0m\n\u001b[1;32m   1312\u001b[0m   \u001b[0;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m   return _get_defun_inputs(\n\u001b[0;32m-> 1314\u001b[0;31m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[0;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[1;32m   1386\u001b[0m           placeholder = graph_placeholder(\n\u001b[1;32m   1387\u001b[0m               \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m               name=requested_name)\n\u001b[0m\u001b[1;32m   1389\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m           \u001b[0;31m# Sometimes parameter names are not valid op names, so fall back to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   op = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m     35\u001b[0m       \u001b[0;34m\"Placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       attrs=attrs, name=name)\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_invoke_op_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    693\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    694\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3782\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3784\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3785\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2182\u001b[0m     \u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationNumOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m       \u001b[0mtf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m       \u001b[0moutput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOutputType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference:https://github.com/dennylslee/rock-paper-scissors-DeepRL"
      ],
      "metadata": {
        "id": "wnS_bH2sjs9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmrhuDYA1IgS4sw/7jdfFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}